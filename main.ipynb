{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  scipy.integrate import quad\n",
    "from scipy.special import gamma\n",
    "import differint.differint as df\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DFSGOptimizer(tf.keras.optimizers.Optimizer):\n",
    "    def __init__(self, learning_rate=0.01, a=0, b=1, name=\"DFSGOptimizer\", **kwargs):\n",
    "        \"\"\"Call super().__init__() and use _set_hyper() to store hyperparameters\"\"\"\n",
    "        super().__init__(name, **kwargs)\n",
    "        self._set_hyper(\"learning_rate\", kwargs.get(\"lr\", learning_rate)) # handle lr=learning_rate\n",
    "        self._is_first = True\n",
    "    \n",
    "    def _create_slots(self, var_list):\n",
    "        \"\"\"For each model variable, create the optimizer variable associated with it.\n",
    "        TensorFlow calls these optimizer variables \"slots\".\n",
    "        For momentum optimization, we need one momentum slot per model variable.\n",
    "        \"\"\"\n",
    "        for var in var_list:\n",
    "            self.add_slot(var, \"pv\") #previous variable i.e. weight or bias\n",
    "        for var in var_list:\n",
    "            self.add_slot(var, \"pg\") #previous gradient\n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def _resource_apply_dense(self, grad, var):\n",
    "        \"\"\"Update the slots and perform one optimization step for one model variable\n",
    "        \"\"\"\n",
    "        var_dtype = var.dtype.base_dtype\n",
    "        lr_t = self._decayed_lr(var_dtype) # handle learning rate decay\n",
    "        # new_var_m = var - grad * lr_t\n",
    "        print(\"DEBUG #1\")\n",
    "        new_var_m = var*2\n",
    "        print(\"DEBUG #2\")\n",
    "        print(var.eval())\n",
    "        print(\"DEBUG #2.1\")\n",
    "        roro = var[var<0]\n",
    "        print(roro.shape)\n",
    "        print(\"DEBUG #2.2\")\n",
    "        # new_var_m[var>0] = var[var>0] - lr_t * quad(lambda alpha: (2*(alpha-self.a))/(self.b-self.a)**2 * df.RL(alpha, f, np.zeros((var[var>0].shape[0], )), var[var>0])[-1], a=self.a, b=self.b)[0]\n",
    "        new_var_m = var - grad * lr_t\n",
    "        print(\"DEBUG #3\")\n",
    "        # new_var_m[var<0] = var[var<0] + lr_t * quad(lambda alpha: (2*(alpha-self.a))/(self.b-self.a)**2 * df.RL(alpha, f, var[var<0], np.zeros((var[var<0].shape[0], )))[-1], a=self.a, b=self.b)[0]\n",
    "        print(\"DEBUG #4\")\n",
    "\n",
    "        pv_var = self.get_slot(var, \"pv\")\n",
    "        pg_var = self.get_slot(var, \"pg\")\n",
    "        \n",
    "        if self._is_first:\n",
    "            self._is_first = False\n",
    "            new_var = new_var_m\n",
    "        else:\n",
    "            cond = grad*pg_var >= 0\n",
    "            print(cond)\n",
    "            avg_weights = (pv_var + var)/2.0\n",
    "            new_var = tf.where(cond, new_var_m, avg_weights)\n",
    "        pv_var.assign(var)\n",
    "        pg_var.assign(grad)\n",
    "        var.assign(new_var)\n",
    "\n",
    "    def _resource_apply_sparse(self, grad, var):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {\n",
    "            **base_config,\n",
    "            \"learning_rate\": self._serialize_hyperparameter(\"learning_rate\"),\n",
    "        }\n",
    "\n",
    "\n",
    "    def _resource_apply_sparse(self, grad, var):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {\n",
    "            **base_config,\n",
    "            \"learning_rate\": self._serialize_hyperparameter(\"learning_rate\"),\n",
    "            \"decay\": self._serialize_hyperparameter(\"decay\"),\n",
    "            \"momentum\": self._serialize_hyperparameter(\"momentum\"),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "DEBUG #1\n",
      "DEBUG #2\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "in user code:\n\n    c:\\Users\\Mohammad\\.conda\\envs\\RL-TF\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:855 train_function  *\n        return step_function(self, iterator)\n    C:\\Users\\Mohammad\\AppData\\Local\\Temp\\ipykernel_17992\\1465144720.py:29 _resource_apply_dense  *\n        print(var.eval())\n    c:\\Users\\Mohammad\\.conda\\envs\\RL-TF\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:624 eval  **\n        return self._graph_element.eval(session=session)\n\n    AttributeError: 'NoneType' object has no attribute 'eval'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Mohammad\\Documents\\Projects\\Fractional\\Untitled-1.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Mohammad/Documents/Projects/Fractional/Untitled-1.ipynb#X42sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mmodels\u001b[39m.\u001b[39mSequential([tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mDense(\u001b[39m10\u001b[39m, input_shape\u001b[39m=\u001b[39m[\u001b[39m8\u001b[39m]), tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mDense(\u001b[39m5\u001b[39m), tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mDense(\u001b[39m1\u001b[39m)])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Mohammad/Documents/Projects/Fractional/Untitled-1.ipynb#X42sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(loss\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmse\u001b[39m\u001b[39m\"\u001b[39m, optimizer\u001b[39m=\u001b[39mDFSGOptimizer(learning_rate\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m))\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Mohammad/Documents/Projects/Fractional/Untitled-1.ipynb#X42sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(X_train_scaled, y_train, epochs\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Mohammad\\.conda\\envs\\RL-TF\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1183\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1176\u001b[0m \u001b[39mwith\u001b[39;00m trace\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1177\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   1178\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   1179\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[0;32m   1180\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[0;32m   1181\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m   1182\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1183\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1184\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1185\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\Mohammad\\.conda\\envs\\RL-TF\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:889\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    886\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    888\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 889\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    891\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    892\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\Mohammad\\.conda\\envs\\RL-TF\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:933\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    931\u001b[0m   \u001b[39m# This is the first call of __call__, so we have to initialize.\u001b[39;00m\n\u001b[0;32m    932\u001b[0m   initializers \u001b[39m=\u001b[39m []\n\u001b[1;32m--> 933\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_initialize(args, kwds, add_initializers_to\u001b[39m=\u001b[39;49minitializers)\n\u001b[0;32m    934\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    935\u001b[0m   \u001b[39m# At this point we know that the initialization is complete (or less\u001b[39;00m\n\u001b[0;32m    936\u001b[0m   \u001b[39m# interestingly an exception was raised) so we no longer need a lock.\u001b[39;00m\n\u001b[0;32m    937\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\Mohammad\\.conda\\envs\\RL-TF\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:763\u001b[0m, in \u001b[0;36mFunction._initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    760\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lifted_initializer_graph \u001b[39m=\u001b[39m lifted_initializer_graph\n\u001b[0;32m    761\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_graph_deleter \u001b[39m=\u001b[39m FunctionDeleter(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lifted_initializer_graph)\n\u001b[0;32m    762\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_concrete_stateful_fn \u001b[39m=\u001b[39m (\n\u001b[1;32m--> 763\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn\u001b[39m.\u001b[39m_get_concrete_function_internal_garbage_collected(  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    764\u001b[0m         \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds))\n\u001b[0;32m    766\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvalid_creator_scope\u001b[39m(\u001b[39m*\u001b[39munused_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39munused_kwds):\n\u001b[0;32m    767\u001b[0m   \u001b[39m\"\"\"Disables variable creation.\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Mohammad\\.conda\\envs\\RL-TF\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3050\u001b[0m, in \u001b[0;36mFunction._get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3048\u001b[0m   args, kwargs \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   3049\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m-> 3050\u001b[0m   graph_function, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_define_function(args, kwargs)\n\u001b[0;32m   3051\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\n",
      "File \u001b[1;32mc:\\Users\\Mohammad\\.conda\\envs\\RL-TF\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3444\u001b[0m, in \u001b[0;36mFunction._maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3440\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_define_function_with_shape_relaxation(\n\u001b[0;32m   3441\u001b[0m       args, kwargs, flat_args, filtered_flat_args, cache_key_context)\n\u001b[0;32m   3443\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function_cache\u001b[39m.\u001b[39mmissed\u001b[39m.\u001b[39madd(call_context_key)\n\u001b[1;32m-> 3444\u001b[0m graph_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_graph_function(args, kwargs)\n\u001b[0;32m   3445\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function_cache\u001b[39m.\u001b[39mprimary[cache_key] \u001b[39m=\u001b[39m graph_function\n\u001b[0;32m   3447\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function, filtered_flat_args\n",
      "File \u001b[1;32mc:\\Users\\Mohammad\\.conda\\envs\\RL-TF\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3279\u001b[0m, in \u001b[0;36mFunction._create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3274\u001b[0m missing_arg_names \u001b[39m=\u001b[39m [\n\u001b[0;32m   3275\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (arg, i) \u001b[39mfor\u001b[39;00m i, arg \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(missing_arg_names)\n\u001b[0;32m   3276\u001b[0m ]\n\u001b[0;32m   3277\u001b[0m arg_names \u001b[39m=\u001b[39m base_arg_names \u001b[39m+\u001b[39m missing_arg_names\n\u001b[0;32m   3278\u001b[0m graph_function \u001b[39m=\u001b[39m ConcreteFunction(\n\u001b[1;32m-> 3279\u001b[0m     func_graph_module\u001b[39m.\u001b[39;49mfunc_graph_from_py_func(\n\u001b[0;32m   3280\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_name,\n\u001b[0;32m   3281\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_python_function,\n\u001b[0;32m   3282\u001b[0m         args,\n\u001b[0;32m   3283\u001b[0m         kwargs,\n\u001b[0;32m   3284\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_signature,\n\u001b[0;32m   3285\u001b[0m         autograph\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_autograph,\n\u001b[0;32m   3286\u001b[0m         autograph_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_autograph_options,\n\u001b[0;32m   3287\u001b[0m         arg_names\u001b[39m=\u001b[39;49marg_names,\n\u001b[0;32m   3288\u001b[0m         override_flat_arg_shapes\u001b[39m=\u001b[39;49moverride_flat_arg_shapes,\n\u001b[0;32m   3289\u001b[0m         capture_by_value\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_capture_by_value),\n\u001b[0;32m   3290\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function_attributes,\n\u001b[0;32m   3291\u001b[0m     function_spec\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction_spec,\n\u001b[0;32m   3292\u001b[0m     \u001b[39m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[39;00m\n\u001b[0;32m   3293\u001b[0m     \u001b[39m# scope. This is not the default behavior since it gets used in some\u001b[39;00m\n\u001b[0;32m   3294\u001b[0m     \u001b[39m# places (like Keras) where the FuncGraph lives longer than the\u001b[39;00m\n\u001b[0;32m   3295\u001b[0m     \u001b[39m# ConcreteFunction.\u001b[39;00m\n\u001b[0;32m   3296\u001b[0m     shared_func_graph\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m   3297\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\n",
      "File \u001b[1;32mc:\\Users\\Mohammad\\.conda\\envs\\RL-TF\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:999\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    996\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    997\u001b[0m   _, original_func \u001b[39m=\u001b[39m tf_decorator\u001b[39m.\u001b[39munwrap(python_func)\n\u001b[1;32m--> 999\u001b[0m func_outputs \u001b[39m=\u001b[39m python_func(\u001b[39m*\u001b[39mfunc_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfunc_kwargs)\n\u001b[0;32m   1001\u001b[0m \u001b[39m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[0;32m   1002\u001b[0m \u001b[39m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[0;32m   1003\u001b[0m func_outputs \u001b[39m=\u001b[39m nest\u001b[39m.\u001b[39mmap_structure(convert, func_outputs,\n\u001b[0;32m   1004\u001b[0m                                   expand_composites\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Mohammad\\.conda\\envs\\RL-TF\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:672\u001b[0m, in \u001b[0;36mFunction._defun_with_scope.<locals>.wrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    668\u001b[0m \u001b[39mwith\u001b[39;00m default_graph\u001b[39m.\u001b[39m_variable_creator_scope(scope, priority\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m):  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    669\u001b[0m   \u001b[39m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[39;00m\n\u001b[0;32m    670\u001b[0m   \u001b[39m# the function a weak reference to itself to avoid a reference cycle.\u001b[39;00m\n\u001b[0;32m    671\u001b[0m   \u001b[39mwith\u001b[39;00m OptionalXlaContext(compile_with_xla):\n\u001b[1;32m--> 672\u001b[0m     out \u001b[39m=\u001b[39m weak_wrapped_fn()\u001b[39m.\u001b[39m__wrapped__(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    673\u001b[0m   \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\Mohammad\\.conda\\envs\\RL-TF\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:986\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    984\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m    985\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 986\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mag_error_metadata\u001b[39m.\u001b[39mto_exception(e)\n\u001b[0;32m    987\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    988\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: in user code:\n\n    c:\\Users\\Mohammad\\.conda\\envs\\RL-TF\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:855 train_function  *\n        return step_function(self, iterator)\n    C:\\Users\\Mohammad\\AppData\\Local\\Temp\\ipykernel_17992\\1465144720.py:29 _resource_apply_dense  *\n        print(var.eval())\n    c:\\Users\\Mohammad\\.conda\\envs\\RL-TF\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:624 eval  **\n        return self._graph_element.eval(session=session)\n\n    AttributeError: 'NoneType' object has no attribute 'eval'\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([tf.keras.layers.Dense(10, input_shape=[8]), tf.keras.layers.Dense(5), tf.keras.layers.Dense(1)])\n",
    "model.compile(loss=\"mse\", optimizer=DFSGOptimizer(learning_rate=0.001))\n",
    "model.fit(X_train_scaled, y_train, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 5\n",
    "for ep in range(epoch):\n",
    "    for i in range(len(y_train)):\n",
    "        w = model.get_weights()\n",
    "        w = dist_frac_optimizer(w)\n",
    "        model.set_weights(w)\n",
    "        out = model.predict(X_train_scaled[i])\n",
    "        w = dist_frac_optimizer(loss, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Flatten\n",
    "from  scipy.integrate import quad\n",
    "import differint.differint as df\n",
    "import itertools\n",
    "import copy\n",
    "\n",
    "class Net:\n",
    "    def __init__(self, x, y, layers, batch_size=32):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.layers = layers\n",
    "        self.w = self.init_weight(self.x, self.layers)\n",
    "        self.w_shape = np.asarray([x.shape for x in self.w])\n",
    "        self.w_flatten = self.flattener(self.w, self.w_shape)\n",
    "        self.step=0\n",
    "        self.batch_size = batch_size\n",
    "        self.batch_counter = 0\n",
    "\n",
    "    def init_weight(self, x, layers):\n",
    "        w = []\n",
    "        tmp = np.random.normal(0, 1, (x.shape[1], layers[0]))\n",
    "        w.append(tmp / np.linalg.norm(tmp))\n",
    "        for i in range(1, len(layers)):\n",
    "            tmp = np.random.normal(0, 1, (layers[i-1], layers[i]))\n",
    "            w.append(tmp / np.linalg.norm(tmp))\n",
    "\n",
    "        return np.asarray(w)\n",
    "\n",
    "    def flattener(self, w, w_shape):\n",
    "        temp = [[xx for x in w[i] for xx in x] for i in range(len(w_shape))]\n",
    "        flat_w = [xx for x in temp for xx in x]\n",
    "        return np.asarray(flat_w)\n",
    "\n",
    "    def forward(self, w):\n",
    "        # print(\"forward calculate\")\n",
    "        H = self.x[self.batch_counter: self.batch_counter + self.batch_size] @ np.asarray(np.reshape(w[:self.w_shape[0][0] * self.w_shape[0][1]], self.w_shape[0]))\n",
    "        for i in range(1, len(self.layers)):\n",
    "            H = H @ np.asarray(np.reshape(w[self.w_shape[i-1][0] * self.w_shape[i-1][1]:(self.w_shape[i-1][0] * self.w_shape[i-1][1]) + (self.w_shape[i][0] * self.w_shape[i][1])], self.w_shape[i]))\n",
    "\n",
    "        return H\n",
    "\n",
    "    def forward_less(self):\n",
    "        H = self.x[self.batch_counter: self.batch_counter + self.batch_size] @ np.asarray(np.reshape(self.w_flatten[:self.w_shape[0][0] * self.w_shape[0][1]], self.w_shape[0]))\n",
    "        for i in range(1, len(self.layers)):\n",
    "            H = H @ np.asarray(np.reshape(self.w_flatten[self.w_shape[i-1][0] * self.w_shape[i-1][1]:(self.w_shape[i-1][0] * self.w_shape[i-1][1]) + (self.w_shape[i][0] * self.w_shape[i][1])], self.w_shape[i]))\n",
    "\n",
    "        return H\n",
    "\n",
    "    def softmax(self, output):\n",
    "        # print(\"softmax calculate\")\n",
    "        return (np.exp(output.T)/np.exp(output.T).sum(axis=0)).T\n",
    "        # return np.asarray([np.asarray([np.exp(one)/ np.sum(np.exp(one_output)) for one in one_output]) for one_output in output])\n",
    "    \n",
    "    def MSE(self, w):\n",
    "        return np.sum((self.y[self.batch_counter: self.batch_counter + self.batch_size] - self.softmax(self.forward())) ** 2)\n",
    "\n",
    "    def categorical_cross_entropy(self, w):\n",
    "        # we use this ([predict[i][np.argmax(target)]if predict[i][np.argmax(target)] != 0 else 10 ** 6]) for handling log(0) and preventing -Inf error.\n",
    "        # print(\"loss calculate\")\n",
    "        # predict = self.softmax(self.forward(w))\n",
    "        predict = self.softmax(self.forward_less())\n",
    "        loss = -(1/self.batch_size) * np.sum([target * np.log([predict[i][np.argmax(target)]if predict[i][np.argmax(target)] != 0 else 0.1 ** 14]) for i, target in enumerate(self.y[self.batch_counter: self.batch_counter + self.batch_size])])\n",
    "        # self.batch_counter += self.batch_size\n",
    "        # print(\"batch_couter: \", self.batch_counter)\n",
    "        return  loss\n",
    "\n",
    "    def grad(self, f, x, epsilon=0.0001):\n",
    "        # print(\"grad calculate\")\n",
    "        return (f(x + epsilon) - f(x)) / epsilon\n",
    "\n",
    "    def optimizer(self, f, x0, lr=0.1, max_iter=100, return_history=False):\n",
    "        x = x0\n",
    "        history = [x]\n",
    "        for i in range(max_iter):\n",
    "            print(f\"Iteration of Opt is: {i}, Loss is: {f(x)}\")\n",
    "            x_new = x - lr * self.grad(f, x)\n",
    "\n",
    "            if f(x_new) < f(x):\n",
    "                x = x_new\n",
    "                self.w_flatten = copy.deepcopy(x_new)\n",
    "            else:\n",
    "                print(f\"Updating learning rate: {lr}\")\n",
    "                lr = 0.1*lr\n",
    "                if lr < 0.1 ** 10:\n",
    "                    print(\"Early STOP!!!\")\n",
    "                    break\n",
    "                \n",
    "            history.append(x)\n",
    "        if return_history:\n",
    "            return x, history\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def frac_optimizer(self, f, x0, lr=0.1, alpha=0.9, max_iter=32, return_history=False):\n",
    "        x = x0\n",
    "        history = [x]\n",
    "        x_new = copy.deepcopy(x)\n",
    "        for i in range(max_iter):\n",
    "            print(f\"Iteration of Opt is: {i}, Loss is: {f(x)}\")\n",
    "            if (x>0).sum()>0:\n",
    "                x_new[x>0] = x[x>0] - lr * df.RL(alpha, f, np.zeros((x.shape[0], )), x)[-1]\n",
    "            if (x<0).sum()>0:                \n",
    "                x_new[x<0] = x[x<0] + lr * df.RL(alpha, f, x, np.zeros((x.shape[0], )))[-1]\n",
    "\n",
    "            if f(x_new) < f(x):\n",
    "                x = x_new\n",
    "                self.w_flatten = copy.deepcopy(x_new)\n",
    "            else:\n",
    "                print(f\"Updating learning rate: {lr}\")\n",
    "                lr = 0.1*lr\n",
    "                if lr < 0.1 ** 10:\n",
    "                    print(\"Early STOP!!!\")\n",
    "                    break\n",
    "            history.append(x)\n",
    "            print(x)\n",
    "        if return_history:\n",
    "            return x, history\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "\n",
    "    def DFoptimizer(self, f,w0, lr=0.1, max_iter=100, a=0.3, b=0.95, return_history=False):\n",
    "        print(f\"Step is: {self.step}\")\n",
    "        self.step += 1\n",
    "        x = w0\n",
    "        x_new = copy.deepcopy(x)\n",
    "        history = [x]\n",
    "        for i in range(max_iter):\n",
    "            print(f\"Iteration of Opt is: {i}\")\n",
    "            x_new[x>0] = x[x>0] - lr * quad(lambda alpha: (2*(alpha-a))/(b-a)**2 * df.RL(alpha, f, np.zeros(x[x>0].shape, ), x[x>0])[-1], a=a, b=b)[0]\n",
    "            x_new[x<0] = x[x<0] + lr * quad(lambda alpha: (2*(alpha-a))/(b-a)**2 * df.RL(alpha, f, x[x<0], np.zeros(x[x>0].shape, ))[-1], a=a, b=b)[0]\n",
    "            if f(x_new) < f(x):\n",
    "                x = x_new\n",
    "            else:\n",
    "                lr = 0.1*lr\n",
    "            history.append(x)\n",
    "        if return_history:\n",
    "            return x, history\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mohammad\\.conda\\envs\\RL-TF\\lib\\site-packages\\differint\\differint.py:324: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  RL = step_size**-alpha*np.dot(D, f_values)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan])"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.RL(0.95, model.categorical_cross_entropy, np.zeros((model.w_flatten.shape[0], )), model.w_flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "203264"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.w_flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.python import metrics\n",
    "\n",
    "def load_mnist():\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    x_train = np.reshape(x_train, [-1, 28*28])\n",
    "    x_test = np.reshape(x_test, [-1, 28*28])\n",
    "    y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "    y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "    x_train = x_train/np.max(x_train)\n",
    "    x_test = x_test/np.max(x_test)\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "def run_tf_model(x_train, y_train, x_test, y_test):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, activation='relu', input_shape=(784,)))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "    print(\"\\nFitting Model...\")\n",
    "    model.fit(x_train, y_train, batch_size=32, epochs=5, validation_split=0.2)\n",
    "    print(\"\\nEvaluating Model...\")\n",
    "    model.evaluate(x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mohammad\\AppData\\Local\\Temp\\ipykernel_5268\\4212714316.py:27: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.asarray(w)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[784, 256],\n",
       "       [256,  10]])"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test= load_mnist()\n",
    "model = Net(x_train, y_train, [256, 10], batch_size=8)\n",
    "model.w_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.forward(w=model.w_flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_soft = model.softmax(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3025912522358247"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.categorical_cross_entropy(w=model.w_flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_grad = model.grad(f=model.categorical_cross_entropy, x=model.w_flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.552476874315701"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grad optimizer\n",
    "epoch = 5\n",
    "for ep in range(epoch):\n",
    "    print(\"EPOCH: \", ep)\n",
    "    print(model.optimizer(model.categorical_cross_entropy, model.w_flatten))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH:  0\n",
      "Batch step:  0\n",
      "Iteration of Opt is: 0, Loss is: 2.3025912522358247\n",
      "Updating learning rate: 0.1\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 1, Loss is: 2.3025912522358247\n",
      "Updating learning rate: 0.010000000000000002\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 2, Loss is: 2.3025912522358247\n",
      "Updating learning rate: 0.0010000000000000002\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 3, Loss is: 2.3025912522358247\n",
      "Updating learning rate: 0.00010000000000000003\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 4, Loss is: 2.3025912522358247\n",
      "Updating learning rate: 1.0000000000000004e-05\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 5, Loss is: 2.3025912522358247\n",
      "Updating learning rate: 1.0000000000000004e-06\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 6, Loss is: 2.3025912522358247\n",
      "Updating learning rate: 1.0000000000000005e-07\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 7, Loss is: 2.3025912522358247\n",
      "Updating learning rate: 1.0000000000000005e-08\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 8, Loss is: 2.3025912522358247\n",
      "Updating learning rate: 1.0000000000000005e-09\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 9, Loss is: 2.3025912522358247\n",
      "Updating learning rate: 1.0000000000000006e-10\n",
      "Early STOP!!!\n",
      "Batch step:  1\n",
      "Iteration of Opt is: 0, Loss is: 2.3014658250396316\n",
      "Updating learning rate: 0.1\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 1, Loss is: 2.3014658250396316\n",
      "Updating learning rate: 0.010000000000000002\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 2, Loss is: 2.3014658250396316\n",
      "Updating learning rate: 0.0010000000000000002\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 3, Loss is: 2.3014658250396316\n",
      "Updating learning rate: 0.00010000000000000003\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 4, Loss is: 2.3014658250396316\n",
      "Updating learning rate: 1.0000000000000004e-05\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 5, Loss is: 2.3014658250396316\n",
      "Updating learning rate: 1.0000000000000004e-06\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 6, Loss is: 2.3014658250396316\n",
      "Updating learning rate: 1.0000000000000005e-07\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 7, Loss is: 2.3014658250396316\n",
      "Updating learning rate: 1.0000000000000005e-08\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 8, Loss is: 2.3014658250396316\n",
      "Updating learning rate: 1.0000000000000005e-09\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 9, Loss is: 2.3014658250396316\n",
      "Updating learning rate: 1.0000000000000006e-10\n",
      "Early STOP!!!\n",
      "Batch step:  2\n",
      "Iteration of Opt is: 0, Loss is: 2.3048634082464337\n",
      "Updating learning rate: 0.1\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 1, Loss is: 2.3048634082464337\n",
      "Updating learning rate: 0.010000000000000002\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 2, Loss is: 2.3048634082464337\n",
      "Updating learning rate: 0.0010000000000000002\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 3, Loss is: 2.3048634082464337\n",
      "Updating learning rate: 0.00010000000000000003\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 4, Loss is: 2.3048634082464337\n",
      "Updating learning rate: 1.0000000000000004e-05\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 5, Loss is: 2.3048634082464337\n",
      "Updating learning rate: 1.0000000000000004e-06\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 6, Loss is: 2.3048634082464337\n",
      "Updating learning rate: 1.0000000000000005e-07\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 7, Loss is: 2.3048634082464337\n",
      "Updating learning rate: 1.0000000000000005e-08\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 8, Loss is: 2.3048634082464337\n",
      "Updating learning rate: 1.0000000000000005e-09\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 9, Loss is: 2.3048634082464337\n",
      "Updating learning rate: 1.0000000000000006e-10\n",
      "Early STOP!!!\n",
      "Batch step:  3\n",
      "Iteration of Opt is: 0, Loss is: 2.305278808607216\n",
      "Updating learning rate: 0.1\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 1, Loss is: 2.305278808607216\n",
      "Updating learning rate: 0.010000000000000002\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 2, Loss is: 2.305278808607216\n",
      "Updating learning rate: 0.0010000000000000002\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 3, Loss is: 2.305278808607216\n",
      "Updating learning rate: 0.00010000000000000003\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 4, Loss is: 2.305278808607216\n",
      "Updating learning rate: 1.0000000000000004e-05\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 5, Loss is: 2.305278808607216\n",
      "Updating learning rate: 1.0000000000000004e-06\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 6, Loss is: 2.305278808607216\n",
      "Updating learning rate: 1.0000000000000005e-07\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 7, Loss is: 2.305278808607216\n",
      "Updating learning rate: 1.0000000000000005e-08\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 8, Loss is: 2.305278808607216\n",
      "Updating learning rate: 1.0000000000000005e-09\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 9, Loss is: 2.305278808607216\n",
      "Updating learning rate: 1.0000000000000006e-10\n",
      "Early STOP!!!\n",
      "Batch step:  4\n",
      "Iteration of Opt is: 0, Loss is: 2.3032443354854126\n",
      "Updating learning rate: 0.1\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 1, Loss is: 2.3032443354854126\n",
      "Updating learning rate: 0.010000000000000002\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 2, Loss is: 2.3032443354854126\n",
      "Updating learning rate: 0.0010000000000000002\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 3, Loss is: 2.3032443354854126\n",
      "Updating learning rate: 0.00010000000000000003\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 4, Loss is: 2.3032443354854126\n",
      "Updating learning rate: 1.0000000000000004e-05\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 5, Loss is: 2.3032443354854126\n",
      "Updating learning rate: 1.0000000000000004e-06\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 6, Loss is: 2.3032443354854126\n",
      "Updating learning rate: 1.0000000000000005e-07\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 7, Loss is: 2.3032443354854126\n",
      "Updating learning rate: 1.0000000000000005e-08\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 8, Loss is: 2.3032443354854126\n",
      "Updating learning rate: 1.0000000000000005e-09\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 9, Loss is: 2.3032443354854126\n",
      "Updating learning rate: 1.0000000000000006e-10\n",
      "Early STOP!!!\n",
      "Batch step:  5\n",
      "Iteration of Opt is: 0, Loss is: 2.3009540946952756\n",
      "Updating learning rate: 0.1\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 1, Loss is: 2.3009540946952756\n",
      "Updating learning rate: 0.010000000000000002\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 2, Loss is: 2.3009540946952756\n",
      "Updating learning rate: 0.0010000000000000002\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 3, Loss is: 2.3009540946952756\n",
      "Updating learning rate: 0.00010000000000000003\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 4, Loss is: 2.3009540946952756\n",
      "Updating learning rate: 1.0000000000000004e-05\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 5, Loss is: 2.3009540946952756\n",
      "Updating learning rate: 1.0000000000000004e-06\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 6, Loss is: 2.3009540946952756\n",
      "Updating learning rate: 1.0000000000000005e-07\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 7, Loss is: 2.3009540946952756\n",
      "Updating learning rate: 1.0000000000000005e-08\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 8, Loss is: 2.3009540946952756\n",
      "Updating learning rate: 1.0000000000000005e-09\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 9, Loss is: 2.3009540946952756\n",
      "Updating learning rate: 1.0000000000000006e-10\n",
      "Early STOP!!!\n",
      "Batch step:  6\n",
      "Iteration of Opt is: 0, Loss is: 2.2986363153658917\n",
      "Updating learning rate: 0.1\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 1, Loss is: 2.2986363153658917\n",
      "Updating learning rate: 0.010000000000000002\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 2, Loss is: 2.2986363153658917\n",
      "Updating learning rate: 0.0010000000000000002\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 3, Loss is: 2.2986363153658917\n",
      "Updating learning rate: 0.00010000000000000003\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 4, Loss is: 2.2986363153658917\n",
      "Updating learning rate: 1.0000000000000004e-05\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 5, Loss is: 2.2986363153658917\n",
      "Updating learning rate: 1.0000000000000004e-06\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 6, Loss is: 2.2986363153658917\n",
      "Updating learning rate: 1.0000000000000005e-07\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 7, Loss is: 2.2986363153658917\n",
      "Updating learning rate: 1.0000000000000005e-08\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 8, Loss is: 2.2986363153658917\n",
      "Updating learning rate: 1.0000000000000005e-09\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 9, Loss is: 2.2986363153658917\n",
      "Updating learning rate: 1.0000000000000006e-10\n",
      "Early STOP!!!\n",
      "Batch step:  7\n",
      "Iteration of Opt is: 0, Loss is: 2.3043659746869025\n",
      "Updating learning rate: 0.1\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 1, Loss is: 2.3043659746869025\n",
      "Updating learning rate: 0.010000000000000002\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 2, Loss is: 2.3043659746869025\n",
      "Updating learning rate: 0.0010000000000000002\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 3, Loss is: 2.3043659746869025\n",
      "Updating learning rate: 0.00010000000000000003\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 4, Loss is: 2.3043659746869025\n",
      "Updating learning rate: 1.0000000000000004e-05\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 5, Loss is: 2.3043659746869025\n",
      "Updating learning rate: 1.0000000000000004e-06\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 6, Loss is: 2.3043659746869025\n",
      "Updating learning rate: 1.0000000000000005e-07\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 7, Loss is: 2.3043659746869025\n",
      "Updating learning rate: 1.0000000000000005e-08\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 8, Loss is: 2.3043659746869025\n",
      "Updating learning rate: 1.0000000000000005e-09\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 9, Loss is: 2.3043659746869025\n",
      "Updating learning rate: 1.0000000000000006e-10\n",
      "Early STOP!!!\n",
      "Batch step:  8\n",
      "Iteration of Opt is: 0, Loss is: 2.3032417096632885\n",
      "Updating learning rate: 0.1\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 1, Loss is: 2.3032417096632885\n",
      "Updating learning rate: 0.010000000000000002\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 2, Loss is: 2.3032417096632885\n",
      "Updating learning rate: 0.0010000000000000002\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 3, Loss is: 2.3032417096632885\n",
      "Updating learning rate: 0.00010000000000000003\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 4, Loss is: 2.3032417096632885\n",
      "Updating learning rate: 1.0000000000000004e-05\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 5, Loss is: 2.3032417096632885\n",
      "Updating learning rate: 1.0000000000000004e-06\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 6, Loss is: 2.3032417096632885\n",
      "Updating learning rate: 1.0000000000000005e-07\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 7, Loss is: 2.3032417096632885\n",
      "Updating learning rate: 1.0000000000000005e-08\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 8, Loss is: 2.3032417096632885\n",
      "Updating learning rate: 1.0000000000000005e-09\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 9, Loss is: 2.3032417096632885\n",
      "Updating learning rate: 1.0000000000000006e-10\n",
      "Early STOP!!!\n",
      "Batch step:  9\n",
      "Iteration of Opt is: 0, Loss is: 2.305113178448557\n",
      "Updating learning rate: 0.1\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 1, Loss is: 2.305113178448557\n",
      "Updating learning rate: 0.010000000000000002\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 2, Loss is: 2.305113178448557\n",
      "Updating learning rate: 0.0010000000000000002\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 3, Loss is: 2.305113178448557\n",
      "Updating learning rate: 0.00010000000000000003\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 4, Loss is: 2.305113178448557\n",
      "Updating learning rate: 1.0000000000000004e-05\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 5, Loss is: 2.305113178448557\n",
      "Updating learning rate: 1.0000000000000004e-06\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 6, Loss is: 2.305113178448557\n",
      "Updating learning rate: 1.0000000000000005e-07\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 7, Loss is: 2.305113178448557\n",
      "Updating learning rate: 1.0000000000000005e-08\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 8, Loss is: 2.305113178448557\n",
      "Updating learning rate: 1.0000000000000005e-09\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 9, Loss is: 2.305113178448557\n",
      "Updating learning rate: 1.0000000000000006e-10\n",
      "Early STOP!!!\n",
      "Batch step:  10\n",
      "Iteration of Opt is: 0, Loss is: 2.303880848592137\n",
      "Updating learning rate: 0.1\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 1, Loss is: 2.303880848592137\n",
      "Updating learning rate: 0.010000000000000002\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 2, Loss is: 2.303880848592137\n",
      "Updating learning rate: 0.0010000000000000002\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 3, Loss is: 2.303880848592137\n",
      "Updating learning rate: 0.00010000000000000003\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 4, Loss is: 2.303880848592137\n",
      "Updating learning rate: 1.0000000000000004e-05\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 5, Loss is: 2.303880848592137\n",
      "Updating learning rate: 1.0000000000000004e-06\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 6, Loss is: 2.303880848592137\n",
      "Updating learning rate: 1.0000000000000005e-07\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 7, Loss is: 2.303880848592137\n",
      "Updating learning rate: 1.0000000000000005e-08\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 8, Loss is: 2.303880848592137\n",
      "Updating learning rate: 1.0000000000000005e-09\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 9, Loss is: 2.303880848592137\n",
      "Updating learning rate: 1.0000000000000006e-10\n",
      "Early STOP!!!\n",
      "Batch step:  11\n",
      "Iteration of Opt is: 0, Loss is: 2.303498491614789\n",
      "Updating learning rate: 0.1\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 1, Loss is: 2.303498491614789\n",
      "Updating learning rate: 0.010000000000000002\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 2, Loss is: 2.303498491614789\n",
      "Updating learning rate: 0.0010000000000000002\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 3, Loss is: 2.303498491614789\n",
      "Updating learning rate: 0.00010000000000000003\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 4, Loss is: 2.303498491614789\n",
      "Updating learning rate: 1.0000000000000004e-05\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 5, Loss is: 2.303498491614789\n",
      "Updating learning rate: 1.0000000000000004e-06\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 6, Loss is: 2.303498491614789\n",
      "Updating learning rate: 1.0000000000000005e-07\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 7, Loss is: 2.303498491614789\n",
      "Updating learning rate: 1.0000000000000005e-08\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 8, Loss is: 2.303498491614789\n",
      "Updating learning rate: 1.0000000000000005e-09\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 9, Loss is: 2.303498491614789\n",
      "Updating learning rate: 1.0000000000000006e-10\n",
      "Early STOP!!!\n",
      "Batch step:  12\n",
      "Iteration of Opt is: 0, Loss is: 2.297169963600031\n",
      "Updating learning rate: 0.1\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 1, Loss is: 2.297169963600031\n",
      "Updating learning rate: 0.010000000000000002\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 2, Loss is: 2.297169963600031\n",
      "Updating learning rate: 0.0010000000000000002\n",
      "[-0.00193344 -0.00014224  0.00150529 ...  0.03034891  0.01840495\n",
      " -0.01418159]\n",
      "Iteration of Opt is: 3, Loss is: 2.297169963600031\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Mohammad\\Documents\\Projects\\Fractional\\Untitled-1.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Mohammad/Documents/Projects/Fractional/Untitled-1.ipynb#Y166sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m b \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mint\u001b[39m(x_test\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\u001b[39m/\u001b[39mmodel\u001b[39m.\u001b[39mbatch_size)):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Mohammad/Documents/Projects/Fractional/Untitled-1.ipynb#Y166sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBatch step: \u001b[39m\u001b[39m\"\u001b[39m, b)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Mohammad/Documents/Projects/Fractional/Untitled-1.ipynb#Y166sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     model\u001b[39m.\u001b[39;49mfrac_optimizer(model\u001b[39m.\u001b[39;49mcategorical_cross_entropy, model\u001b[39m.\u001b[39;49mw_flatten)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Mohammad/Documents/Projects/Fractional/Untitled-1.ipynb#Y166sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     model\u001b[39m.\u001b[39mbatch_counter \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mbatch_size\n",
      "\u001b[1;32mc:\\Users\\Mohammad\\Documents\\Projects\\Fractional\\Untitled-1.ipynb Cell 14\u001b[0m in \u001b[0;36mNet.frac_optimizer\u001b[1;34m(self, f, x0, lr, alpha, max_iter, return_history)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Mohammad/Documents/Projects/Fractional/Untitled-1.ipynb#Y166sZmlsZQ%3D%3D?line=99'>100</a>\u001b[0m     x_new[x\u001b[39m>\u001b[39m\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m x[x\u001b[39m>\u001b[39m\u001b[39m0\u001b[39m] \u001b[39m-\u001b[39m lr \u001b[39m*\u001b[39m df\u001b[39m.\u001b[39mRL(alpha, f, np\u001b[39m.\u001b[39mzeros((x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], )), x)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Mohammad/Documents/Projects/Fractional/Untitled-1.ipynb#Y166sZmlsZQ%3D%3D?line=100'>101</a>\u001b[0m \u001b[39mif\u001b[39;00m (x\u001b[39m<\u001b[39m\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39msum()\u001b[39m>\u001b[39m\u001b[39m0\u001b[39m:                \n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/Mohammad/Documents/Projects/Fractional/Untitled-1.ipynb#Y166sZmlsZQ%3D%3D?line=101'>102</a>\u001b[0m     x_new[x\u001b[39m<\u001b[39m\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m x[x\u001b[39m<\u001b[39m\u001b[39m0\u001b[39m] \u001b[39m+\u001b[39m lr \u001b[39m*\u001b[39m df\u001b[39m.\u001b[39;49mRL(alpha, f, x, np\u001b[39m.\u001b[39;49mzeros((x\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m], )))[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Mohammad/Documents/Projects/Fractional/Untitled-1.ipynb#Y166sZmlsZQ%3D%3D?line=103'>104</a>\u001b[0m \u001b[39mif\u001b[39;00m f(x_new) \u001b[39m<\u001b[39m f(x):\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Mohammad/Documents/Projects/Fractional/Untitled-1.ipynb#Y166sZmlsZQ%3D%3D?line=104'>105</a>\u001b[0m     x \u001b[39m=\u001b[39m x_new\n",
      "File \u001b[1;32mc:\\Users\\Mohammad\\.conda\\envs\\RL-TF\\lib\\site-packages\\differint\\differint.py:319\u001b[0m, in \u001b[0;36mRL\u001b[1;34m(alpha, f_name, domain_start, domain_end, num_points)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39m\"\"\" Calculate the RL algorithm using a trapezoid rule over \u001b[39;00m\n\u001b[0;32m    285\u001b[0m \u001b[39m    an array of function values.\u001b[39;00m\n\u001b[0;32m    286\u001b[0m \u001b[39m    \u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[39m    >>> RL_poly = RL(0.5, lambda x: x**2 - 1, 0., 1., 100)\u001b[39;00m\n\u001b[0;32m    311\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    313\u001b[0m \u001b[39m# Flip the domain limits if they are in the wrong order.\u001b[39;00m\n\u001b[0;32m    314\u001b[0m \u001b[39m# if domain_start > domain_end: TODO: mychanges\u001b[39;00m\n\u001b[0;32m    315\u001b[0m     \u001b[39m# domain_start, domain_end = domain_end, domain_start TODO: mychanges\u001b[39;00m\n\u001b[0;32m    316\u001b[0m \n\u001b[0;32m    317\u001b[0m \u001b[39m# Check inputs.\u001b[39;00m\n\u001b[0;32m    318\u001b[0m \u001b[39m# checkValues(alpha, domain_start, domain_end, num_points) TODO: mychanges\u001b[39;00m\n\u001b[1;32m--> 319\u001b[0m f_values, step_size \u001b[39m=\u001b[39m functionCheck(f_name, domain_start, domain_end, num_points)\n\u001b[0;32m    320\u001b[0m \u001b[39m# Calculate the RL differintegral.\u001b[39;00m\n\u001b[0;32m    321\u001b[0m \u001b[39m# print(f_values,\"\\n\\n\")\u001b[39;00m\n\u001b[0;32m    322\u001b[0m \u001b[39m# print(np.asarray(f_values).shape)\u001b[39;00m\n\u001b[0;32m    323\u001b[0m D \u001b[39m=\u001b[39m RLmatrix(alpha, num_points)\n",
      "File \u001b[1;32mc:\\Users\\Mohammad\\.conda\\envs\\RL-TF\\lib\\site-packages\\differint\\differint.py:31\u001b[0m, in \u001b[0;36mfunctionCheck\u001b[1;34m(f_name, domain_start, domain_end, num_points)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(f_name, \u001b[39m'\u001b[39m\u001b[39m__call__\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m     29\u001b[0m     \u001b[39m# If f_name is callable, call it and save to a list.\u001b[39;00m\n\u001b[0;32m     30\u001b[0m     x \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlinspace(domain_start, domain_end, num_points)\n\u001b[1;32m---> 31\u001b[0m     f_values \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(\u001b[39mmap\u001b[39;49m(\u001b[39mlambda\u001b[39;49;00m t: f_name(t), x)) \n\u001b[0;32m     32\u001b[0m     step_size \u001b[39m=\u001b[39m x[\u001b[39m1\u001b[39m] \u001b[39m-\u001b[39m x[\u001b[39m0\u001b[39m]\n\u001b[0;32m     33\u001b[0m     \u001b[39m# TODO Modified\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Mohammad\\.conda\\envs\\RL-TF\\lib\\site-packages\\differint\\differint.py:31\u001b[0m, in \u001b[0;36mfunctionCheck.<locals>.<lambda>\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(f_name, \u001b[39m'\u001b[39m\u001b[39m__call__\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m     29\u001b[0m     \u001b[39m# If f_name is callable, call it and save to a list.\u001b[39;00m\n\u001b[0;32m     30\u001b[0m     x \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlinspace(domain_start, domain_end, num_points)\n\u001b[1;32m---> 31\u001b[0m     f_values \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mmap\u001b[39m(\u001b[39mlambda\u001b[39;00m t: f_name(t), x)) \n\u001b[0;32m     32\u001b[0m     step_size \u001b[39m=\u001b[39m x[\u001b[39m1\u001b[39m] \u001b[39m-\u001b[39m x[\u001b[39m0\u001b[39m]\n\u001b[0;32m     33\u001b[0m     \u001b[39m# TODO Modified\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\Mohammad\\Documents\\Projects\\Fractional\\Untitled-1.ipynb Cell 14\u001b[0m in \u001b[0;36mNet.categorical_cross_entropy\u001b[1;34m(self, w)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Mohammad/Documents/Projects/Fractional/Untitled-1.ipynb#Y166sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcategorical_cross_entropy\u001b[39m(\u001b[39mself\u001b[39m, w):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Mohammad/Documents/Projects/Fractional/Untitled-1.ipynb#Y166sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m     \u001b[39m# we use this ([predict[i][np.argmax(target)]if predict[i][np.argmax(target)] != 0 else 10 ** 6]) for handling log(0) and preventing -Inf error.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Mohammad/Documents/Projects/Fractional/Untitled-1.ipynb#Y166sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m     \u001b[39m# print(\"loss calculate\")\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Mohammad/Documents/Projects/Fractional/Untitled-1.ipynb#Y166sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m     predict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msoftmax(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(w))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Mohammad/Documents/Projects/Fractional/Untitled-1.ipynb#Y166sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m(\u001b[39m1\u001b[39m\u001b[39m/\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size) \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39msum([target \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mlog([predict[i][np\u001b[39m.\u001b[39margmax(target)]\u001b[39mif\u001b[39;00m predict[i][np\u001b[39m.\u001b[39margmax(target)] \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m0.1\u001b[39m \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m14\u001b[39m]) \u001b[39mfor\u001b[39;00m i, target \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39my[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_counter: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_counter \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size])])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Mohammad/Documents/Projects/Fractional/Untitled-1.ipynb#Y166sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m     \u001b[39m# self.batch_counter += self.batch_size\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Mohammad/Documents/Projects/Fractional/Untitled-1.ipynb#Y166sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m     \u001b[39m# print(\"batch_couter: \", self.batch_counter)\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\Mohammad\\Documents\\Projects\\Fractional\\Untitled-1.ipynb Cell 14\u001b[0m in \u001b[0;36mNet.forward\u001b[1;34m(self, w)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Mohammad/Documents/Projects/Fractional/Untitled-1.ipynb#Y166sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, w):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Mohammad/Documents/Projects/Fractional/Untitled-1.ipynb#Y166sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     \u001b[39m# print(\"forward calculate\")\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Mohammad/Documents/Projects/Fractional/Untitled-1.ipynb#Y166sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     H \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mx[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_counter: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_counter \u001b[39m+\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_size] \u001b[39m@\u001b[39;49m np\u001b[39m.\u001b[39;49masarray(np\u001b[39m.\u001b[39;49mreshape(w[:\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mw_shape[\u001b[39m0\u001b[39;49m][\u001b[39m0\u001b[39;49m] \u001b[39m*\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mw_shape[\u001b[39m0\u001b[39;49m][\u001b[39m1\u001b[39;49m]], \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mw_shape[\u001b[39m0\u001b[39;49m]))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Mohammad/Documents/Projects/Fractional/Untitled-1.ipynb#Y166sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers)):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Mohammad/Documents/Projects/Fractional/Untitled-1.ipynb#Y166sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m         H \u001b[39m=\u001b[39m H \u001b[39m@\u001b[39m np\u001b[39m.\u001b[39masarray(np\u001b[39m.\u001b[39mreshape(w[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mw_shape[i\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m] \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mw_shape[i\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m1\u001b[39m]:(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mw_shape[i\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m] \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mw_shape[i\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m1\u001b[39m]) \u001b[39m+\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mw_shape[i][\u001b[39m0\u001b[39m] \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mw_shape[i][\u001b[39m1\u001b[39m])], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mw_shape[i]))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# frac_optimizer\n",
    "epoch = 5\n",
    "for ep in range(epoch):\n",
    "    print(\"EPOCH: \", ep)\n",
    "    for b in range(int(x_test.shape[0]/model.batch_size)):\n",
    "        print(\"Batch step: \", b)\n",
    "        model.frac_optimizer(model.categorical_cross_entropy, model.w_flatten)\n",
    "        model.batch_counter += model.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.forward(model.w_flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Net' object has no attribute 'loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Mohammad\\Documents\\Projects\\Fractional\\Untitled-1.ipynb Cell 16\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Mohammad/Documents/Projects/Fractional/Untitled-1.ipynb#X53sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m b\u001b[39m=\u001b[39m\u001b[39m0.95\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Mohammad/Documents/Projects/Fractional/Untitled-1.ipynb#X53sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m x\u001b[39m=\u001b[39mmodel\u001b[39m.\u001b[39mw_flatten\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Mohammad/Documents/Projects/Fractional/Untitled-1.ipynb#X53sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m f\u001b[39m=\u001b[39mmodel\u001b[39m.\u001b[39;49mloss\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Mohammad/Documents/Projects/Fractional/Untitled-1.ipynb#X53sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m quad(\u001b[39mlambda\u001b[39;00m alpha: (\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m(alpha\u001b[39m-\u001b[39ma))\u001b[39m/\u001b[39m(b\u001b[39m-\u001b[39ma)\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m \u001b[39m*\u001b[39m df\u001b[39m.\u001b[39mRL(alpha, f, np\u001b[39m.\u001b[39mzeros(x[x\u001b[39m>\u001b[39m\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape, ), x[x\u001b[39m>\u001b[39m\u001b[39m0\u001b[39m])[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], a\u001b[39m=\u001b[39ma, b\u001b[39m=\u001b[39mb)[\u001b[39m0\u001b[39m], df\u001b[39m.\u001b[39mRL(\u001b[39m0.9\u001b[39m, f, np\u001b[39m.\u001b[39mzeros(x[x\u001b[39m>\u001b[39m\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape, ), x[x\u001b[39m>\u001b[39m\u001b[39m0\u001b[39m])[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], quad(\u001b[39mlambda\u001b[39;00m alpha: (\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m(alpha\u001b[39m-\u001b[39ma))\u001b[39m/\u001b[39m(b\u001b[39m-\u001b[39ma)\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m \u001b[39m*\u001b[39m df\u001b[39m.\u001b[39mRL(alpha, f, x[x\u001b[39m<\u001b[39m\u001b[39m0\u001b[39m], np\u001b[39m.\u001b[39mzeros(x[x\u001b[39m<\u001b[39m\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape, ))[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], a\u001b[39m=\u001b[39ma, b\u001b[39m=\u001b[39mb)[\u001b[39m0\u001b[39m], df\u001b[39m.\u001b[39mRL(\u001b[39m0.9\u001b[39m, f, x[x\u001b[39m<\u001b[39m\u001b[39m0\u001b[39m], np\u001b[39m.\u001b[39mzeros(x[x\u001b[39m<\u001b[39m\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape, ))[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Net' object has no attribute 'loss'"
     ]
    }
   ],
   "source": [
    "a=0.3\n",
    "b=0.95\n",
    "x=model.w_flatten\n",
    "f=model.loss\n",
    "quad(lambda alpha: (2*(alpha-a))/(b-a)**2 * df.RL(alpha, f, np.zeros(x[x>0].shape, ), x[x>0])[-1], a=a, b=b)[0], df.RL(0.9, f, np.zeros(x[x>0].shape, ), x[x>0])[-1], quad(lambda alpha: (2*(alpha-a))/(b-a)**2 * df.RL(alpha, f, x[x<0], np.zeros(x[x<0].shape, ))[-1], a=a, b=b)[0], df.RL(0.9, f, x[x<0], np.zeros(x[x<0].shape, ))[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step is: 0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (19,) (17,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Mohammad\\Documents\\Projects\\Fractional\\Untitled-1.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Mohammad/Documents/Projects/Fractional/Untitled-1.ipynb#X54sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39;49mDFoptimizer(f\u001b[39m=\u001b[39;49mmodel\u001b[39m.\u001b[39;49mloss, w0\u001b[39m=\u001b[39;49mmodel\u001b[39m.\u001b[39;49mw_flatten)\n",
      "\u001b[1;32mc:\\Users\\Mohammad\\Documents\\Projects\\Fractional\\Untitled-1.ipynb Cell 12\u001b[0m in \u001b[0;36mNet.DFoptimizer\u001b[1;34m(self, f, w0, lr, max_iter, a, b, return_history)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Mohammad/Documents/Projects/Fractional/Untitled-1.ipynb#X54sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(max_iter):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Mohammad/Documents/Projects/Fractional/Untitled-1.ipynb#X54sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     x_new[x_new\u001b[39m>\u001b[39m\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m x[x\u001b[39m>\u001b[39m\u001b[39m0\u001b[39m] \u001b[39m-\u001b[39m lr \u001b[39m*\u001b[39m quad(\u001b[39mlambda\u001b[39;00m alpha: (\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m(alpha\u001b[39m-\u001b[39ma))\u001b[39m/\u001b[39m(b\u001b[39m-\u001b[39ma)\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m \u001b[39m*\u001b[39m df\u001b[39m.\u001b[39mRL(alpha, f, np\u001b[39m.\u001b[39mzeros(x[x\u001b[39m>\u001b[39m\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape, ), x[x\u001b[39m>\u001b[39m\u001b[39m0\u001b[39m])[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], a\u001b[39m=\u001b[39ma, b\u001b[39m=\u001b[39mb)[\u001b[39m0\u001b[39m]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Mohammad/Documents/Projects/Fractional/Untitled-1.ipynb#X54sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m     x_new[x_new\u001b[39m<\u001b[39m\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m x[x\u001b[39m<\u001b[39m\u001b[39m0\u001b[39m] \u001b[39m+\u001b[39m lr \u001b[39m*\u001b[39m quad(\u001b[39mlambda\u001b[39;49;00m alpha: (\u001b[39m2\u001b[39;49m\u001b[39m*\u001b[39;49m(alpha\u001b[39m-\u001b[39;49ma))\u001b[39m/\u001b[39;49m(b\u001b[39m-\u001b[39;49ma)\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39m2\u001b[39;49m \u001b[39m*\u001b[39;49m df\u001b[39m.\u001b[39;49mRL(alpha, f, x[x\u001b[39m<\u001b[39;49m\u001b[39m0\u001b[39;49m], np\u001b[39m.\u001b[39;49mzeros(x[x\u001b[39m>\u001b[39;49m\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mshape, ))[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m], a\u001b[39m=\u001b[39;49ma, b\u001b[39m=\u001b[39;49mb)[\u001b[39m0\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Mohammad/Documents/Projects/Fractional/Untitled-1.ipynb#X54sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     \u001b[39mif\u001b[39;00m f(x_new) \u001b[39m<\u001b[39m f(x):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Mohammad/Documents/Projects/Fractional/Untitled-1.ipynb#X54sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m         x \u001b[39m=\u001b[39m x_new\n",
      "File \u001b[1;32mc:\\Users\\Mohammad\\.conda\\envs\\RL-TF\\lib\\site-packages\\scipy\\integrate\\quadpack.py:351\u001b[0m, in \u001b[0;36mquad\u001b[1;34m(func, a, b, args, full_output, epsabs, epsrel, limit, points, weight, wvar, wopts, maxp1, limlst)\u001b[0m\n\u001b[0;32m    348\u001b[0m flip, a, b \u001b[39m=\u001b[39m b \u001b[39m<\u001b[39m a, \u001b[39mmin\u001b[39m(a, b), \u001b[39mmax\u001b[39m(a, b)\n\u001b[0;32m    350\u001b[0m \u001b[39mif\u001b[39;00m weight \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 351\u001b[0m     retval \u001b[39m=\u001b[39m _quad(func, a, b, args, full_output, epsabs, epsrel, limit,\n\u001b[0;32m    352\u001b[0m                    points)\n\u001b[0;32m    353\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    354\u001b[0m     \u001b[39mif\u001b[39;00m points \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Mohammad\\.conda\\envs\\RL-TF\\lib\\site-packages\\scipy\\integrate\\quadpack.py:463\u001b[0m, in \u001b[0;36m_quad\u001b[1;34m(func, a, b, args, full_output, epsabs, epsrel, limit, points)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[39mif\u001b[39;00m points \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    462\u001b[0m     \u001b[39mif\u001b[39;00m infbounds \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> 463\u001b[0m         \u001b[39mreturn\u001b[39;00m _quadpack\u001b[39m.\u001b[39;49m_qagse(func,a,b,args,full_output,epsabs,epsrel,limit)\n\u001b[0;32m    464\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    465\u001b[0m         \u001b[39mreturn\u001b[39;00m _quadpack\u001b[39m.\u001b[39m_qagie(func,bound,infbounds,args,full_output,epsabs,epsrel,limit)\n",
      "\u001b[1;32mc:\\Users\\Mohammad\\Documents\\Projects\\Fractional\\Untitled-1.ipynb Cell 12\u001b[0m in \u001b[0;36mNet.DFoptimizer.<locals>.<lambda>\u001b[1;34m(alpha)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Mohammad/Documents/Projects/Fractional/Untitled-1.ipynb#X54sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(max_iter):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Mohammad/Documents/Projects/Fractional/Untitled-1.ipynb#X54sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     x_new[x_new\u001b[39m>\u001b[39m\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m x[x\u001b[39m>\u001b[39m\u001b[39m0\u001b[39m] \u001b[39m-\u001b[39m lr \u001b[39m*\u001b[39m quad(\u001b[39mlambda\u001b[39;00m alpha: (\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m(alpha\u001b[39m-\u001b[39ma))\u001b[39m/\u001b[39m(b\u001b[39m-\u001b[39ma)\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m \u001b[39m*\u001b[39m df\u001b[39m.\u001b[39mRL(alpha, f, np\u001b[39m.\u001b[39mzeros(x[x\u001b[39m>\u001b[39m\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape, ), x[x\u001b[39m>\u001b[39m\u001b[39m0\u001b[39m])[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], a\u001b[39m=\u001b[39ma, b\u001b[39m=\u001b[39mb)[\u001b[39m0\u001b[39m]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Mohammad/Documents/Projects/Fractional/Untitled-1.ipynb#X54sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m     x_new[x_new\u001b[39m<\u001b[39m\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m x[x\u001b[39m<\u001b[39m\u001b[39m0\u001b[39m] \u001b[39m+\u001b[39m lr \u001b[39m*\u001b[39m quad(\u001b[39mlambda\u001b[39;00m alpha: (\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m(alpha\u001b[39m-\u001b[39ma))\u001b[39m/\u001b[39m(b\u001b[39m-\u001b[39ma)\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m \u001b[39m*\u001b[39m df\u001b[39m.\u001b[39;49mRL(alpha, f, x[x\u001b[39m<\u001b[39;49m\u001b[39m0\u001b[39;49m], np\u001b[39m.\u001b[39;49mzeros(x[x\u001b[39m>\u001b[39;49m\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mshape, ))[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], a\u001b[39m=\u001b[39ma, b\u001b[39m=\u001b[39mb)[\u001b[39m0\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Mohammad/Documents/Projects/Fractional/Untitled-1.ipynb#X54sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     \u001b[39mif\u001b[39;00m f(x_new) \u001b[39m<\u001b[39m f(x):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Mohammad/Documents/Projects/Fractional/Untitled-1.ipynb#X54sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m         x \u001b[39m=\u001b[39m x_new\n",
      "File \u001b[1;32mc:\\Users\\Mohammad\\.conda\\envs\\RL-TF\\lib\\site-packages\\differint\\differint.py:319\u001b[0m, in \u001b[0;36mRL\u001b[1;34m(alpha, f_name, domain_start, domain_end, num_points)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39m\"\"\" Calculate the RL algorithm using a trapezoid rule over \u001b[39;00m\n\u001b[0;32m    285\u001b[0m \u001b[39m    an array of function values.\u001b[39;00m\n\u001b[0;32m    286\u001b[0m \u001b[39m    \u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[39m    >>> RL_poly = RL(0.5, lambda x: x**2 - 1, 0., 1., 100)\u001b[39;00m\n\u001b[0;32m    311\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    313\u001b[0m \u001b[39m# Flip the domain limits if they are in the wrong order.\u001b[39;00m\n\u001b[0;32m    314\u001b[0m \u001b[39m# if domain_start > domain_end: TODO: mychanges\u001b[39;00m\n\u001b[0;32m    315\u001b[0m     \u001b[39m# domain_start, domain_end = domain_end, domain_start TODO: mychanges\u001b[39;00m\n\u001b[0;32m    316\u001b[0m \n\u001b[0;32m    317\u001b[0m \u001b[39m# Check inputs.\u001b[39;00m\n\u001b[0;32m    318\u001b[0m \u001b[39m# checkValues(alpha, domain_start, domain_end, num_points) TODO: mychanges\u001b[39;00m\n\u001b[1;32m--> 319\u001b[0m f_values, step_size \u001b[39m=\u001b[39m functionCheck(f_name, domain_start, domain_end, num_points)\n\u001b[0;32m    320\u001b[0m \u001b[39m# Calculate the RL differintegral.\u001b[39;00m\n\u001b[0;32m    321\u001b[0m \u001b[39m# print(f_values,\"\\n\\n\")\u001b[39;00m\n\u001b[0;32m    322\u001b[0m \u001b[39m# print(np.asarray(f_values).shape)\u001b[39;00m\n\u001b[0;32m    323\u001b[0m D \u001b[39m=\u001b[39m RLmatrix(alpha, num_points)\n",
      "File \u001b[1;32mc:\\Users\\Mohammad\\.conda\\envs\\RL-TF\\lib\\site-packages\\differint\\differint.py:30\u001b[0m, in \u001b[0;36mfunctionCheck\u001b[1;34m(f_name, domain_start, domain_end, num_points)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[39m# Define the function domain and obtain function values.\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(f_name, \u001b[39m'\u001b[39m\u001b[39m__call__\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m     29\u001b[0m     \u001b[39m# If f_name is callable, call it and save to a list.\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m     x \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mlinspace(domain_start, domain_end, num_points)\n\u001b[0;32m     31\u001b[0m     f_values \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mmap\u001b[39m(\u001b[39mlambda\u001b[39;00m t: f_name(t), x)) \n\u001b[0;32m     32\u001b[0m     step_size \u001b[39m=\u001b[39m x[\u001b[39m1\u001b[39m] \u001b[39m-\u001b[39m x[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36mlinspace\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Mohammad\\.conda\\envs\\RL-TF\\lib\\site-packages\\numpy\\core\\function_base.py:134\u001b[0m, in \u001b[0;36mlinspace\u001b[1;34m(start, stop, num, endpoint, retstep, dtype, axis)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    132\u001b[0m     dtype \u001b[39m=\u001b[39m dt\n\u001b[1;32m--> 134\u001b[0m delta \u001b[39m=\u001b[39m stop \u001b[39m-\u001b[39;49m start\n\u001b[0;32m    135\u001b[0m y \u001b[39m=\u001b[39m _nx\u001b[39m.\u001b[39marange(\u001b[39m0\u001b[39m, num, dtype\u001b[39m=\u001b[39mdt)\u001b[39m.\u001b[39mreshape((\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,) \u001b[39m+\u001b[39m (\u001b[39m1\u001b[39m,) \u001b[39m*\u001b[39m ndim(delta))\n\u001b[0;32m    136\u001b[0m \u001b[39m# In-place multiplication y *= delta/div is faster, but prevents the multiplicant\u001b[39;00m\n\u001b[0;32m    137\u001b[0m \u001b[39m# from overriding what class is produced, and thus prevents, e.g. use of Quantities,\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[39m# see gh-7142. Hence, we multiply in place only for standard scalar types.\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (19,) (17,) "
     ]
    }
   ],
   "source": [
    "model.DFoptimizer(f=model.loss, w0=model.w_flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fitting Model...\n",
      "Epoch 1/5\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 0.6828 - accuracy: 0.8339 - val_loss: 0.3713 - val_accuracy: 0.9018\n",
      "Epoch 2/5\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3550 - accuracy: 0.9021 - val_loss: 0.3034 - val_accuracy: 0.9170\n",
      "Epoch 3/5\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3027 - accuracy: 0.9156 - val_loss: 0.2712 - val_accuracy: 0.9241\n",
      "Epoch 4/5\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.2722 - accuracy: 0.9236 - val_loss: 0.2495 - val_accuracy: 0.9304\n",
      "Epoch 5/5\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.2494 - accuracy: 0.9301 - val_loss: 0.2323 - val_accuracy: 0.9352\n",
      "\n",
      "Evaluating Model...\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.2297 - accuracy: 0.9363\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test= load_mnist()\n",
    "run_tf_model(x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = w.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = [dd.tolist() for dd in d][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.98976572, 1.2954313 , 0.        , 0.04755698, 0.67182099,\n",
       "       0.37223025, 0.        , 0.76523655, 0.        , 0.05103283,\n",
       "       0.        , 0.72666734, 0.        , 0.        , 0.        ,\n",
       "       0.86102577, 0.73552149, 0.        , 1.25006392, 0.        ,\n",
       "       0.        , 0.        , 1.36286233, 2.13204624, 1.01869583,\n",
       "       0.        , 0.        , 1.62782528, 0.19672655, 1.88353804,\n",
       "       0.        , 0.34709588, 0.34611852, 0.42329038, 0.        ,\n",
       "       0.7168049 ])"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.w_flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Mohammad\\Documents\\Projects\\Fractional\\Untitled-1.ipynb Cell 45\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Mohammad/Documents/Projects/Fractional/Untitled-1.ipynb#Y123sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m Flatten(w)\n",
      "File \u001b[1;32mc:\\Users\\Mohammad\\.conda\\envs\\RL-TF\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py:657\u001b[0m, in \u001b[0;36mFlatten.__init__\u001b[1;34m(self, data_format, **kwargs)\u001b[0m\n\u001b[0;32m    655\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, data_format\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    656\u001b[0m   \u001b[39msuper\u001b[39m(Flatten, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 657\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_format \u001b[39m=\u001b[39m conv_utils\u001b[39m.\u001b[39;49mnormalize_data_format(data_format)\n\u001b[0;32m    658\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_spec \u001b[39m=\u001b[39m InputSpec(min_ndim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m    659\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_channels_first \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_format \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mchannels_first\u001b[39m\u001b[39m'\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Mohammad\\.conda\\envs\\RL-TF\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\conv_utils.py:191\u001b[0m, in \u001b[0;36mnormalize_data_format\u001b[1;34m(value)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[39mif\u001b[39;00m value \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    190\u001b[0m   value \u001b[39m=\u001b[39m backend\u001b[39m.\u001b[39mimage_data_format()\n\u001b[1;32m--> 191\u001b[0m data_format \u001b[39m=\u001b[39m value\u001b[39m.\u001b[39;49mlower()\n\u001b[0;32m    192\u001b[0m \u001b[39mif\u001b[39;00m data_format \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m {\u001b[39m'\u001b[39m\u001b[39mchannels_first\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mchannels_last\u001b[39m\u001b[39m'\u001b[39m}:\n\u001b[0;32m    193\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mThe `data_format` argument must be one of \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    194\u001b[0m                    \u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mchannels_first\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mchannels_last\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m. Received: \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m\n\u001b[0;32m    195\u001b[0m                    \u001b[39mstr\u001b[39m(value))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "Flatten(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.w[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10,  5],\n",
       "       [ 5,  1]])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.w_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([array([[ 0.04933916,  0.31724101,  1.33483212, -0.36951808,  0.41092349],\n",
       "              [ 0.05559069, -1.16103175, -0.28077493,  1.21152419,  1.14157878],\n",
       "              [ 0.15441872, -0.3125879 ,  0.03497398,  0.1621402 , -1.90035389],\n",
       "              [ 0.7068688 , -0.0317973 , -0.86584421,  0.32372658, -0.89127474],\n",
       "              [-0.10104525,  0.96800556, -1.39178279,  2.27920435,  1.00664723],\n",
       "              [ 0.82722813, -1.07102583,  0.35435052, -0.52659841, -1.37762987],\n",
       "              [ 0.08271013, -0.96096173,  1.13707415, -0.67341641, -0.08163573],\n",
       "              [ 0.17273798, -0.69070635,  1.37545584,  1.24066547,  0.70428081],\n",
       "              [ 0.74952304, -1.11908976,  2.01680357, -1.51747064, -1.90236393],\n",
       "              [-0.5939951 , -1.53041876,  0.97819404, -0.87736361, -1.27146795]]),\n",
       "       array([[ 1.05409205],\n",
       "              [ 0.14346276],\n",
       "              [-0.24564847],\n",
       "              [-0.06150027],\n",
       "              [-0.5658025 ]])], dtype=object)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_f = np.concatenate(model.w).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.float64"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(np.reshape(w_f[:50], model.w_shape[0])[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.07864889,  2.42846981,  1.54135967, -0.66644431, -0.58885281],\n",
       "       [-1.40438002,  0.40655187,  1.42136415, -2.25349914,  0.208802  ],\n",
       "       [-0.85526952, -0.63766747,  0.28649079,  0.31262855, -0.3212233 ],\n",
       "       [ 0.9956536 , -0.44415874, -0.26332207, -1.23753941, -0.55017982],\n",
       "       [ 0.57276725, -2.44569546,  0.17469251, -0.06614036,  0.63622065],\n",
       "       [ 0.67306601,  0.03192776, -0.03073269, -0.69936905,  0.91358389],\n",
       "       [-0.46484078, -0.19981802, -0.62470099, -0.51558878, -0.94093217],\n",
       "       [-0.70036259,  1.27562823,  0.16093373, -0.84742128, -0.33531485],\n",
       "       [-0.01276554,  0.12688483,  0.19269448, -1.08818336, -0.17162264],\n",
       "       [ 1.33220445, -1.06598381, -0.17105323,  1.73033611, -0.54751369]])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.reshape(w_f[:50], model.w_shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 5\n",
    "model = Net()\n",
    "for ep in range(epoch):\n",
    "    for i in range(len(y_train)):\n",
    "        w = dist_frac_optimizer(w)\n",
    "        model.set_weights(w)\n",
    "        out = model.predict(X_train_scaled[i])\n",
    "        w = dist_frac_optimizer(loss, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.random((10,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = np.array([3,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.2406876 , 0.18074978, 0.25898608, 0.43478112, 0.54350592,\n",
       "        0.28659122, 0.20408715, 0.37472006, 0.89721325, 0.75278481],\n",
       "       [0.11015054, 0.43815505, 0.17482847, 0.92732596, 0.14846509,\n",
       "        0.24431521, 0.73363338, 0.1071827 , 0.32321506, 0.58601648]])"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.532588  , 0.467412  ],\n",
       "       [0.43600166, 0.56399834],\n",
       "       [0.52102699, 0.47897301],\n",
       "       [0.37929425, 0.62070575],\n",
       "       [0.59749558, 0.40250442],\n",
       "       [0.51056743, 0.48943257],\n",
       "       [0.37062273, 0.62937727],\n",
       "       [0.56648823, 0.43351177],\n",
       "       [0.63968522, 0.36031478],\n",
       "       [0.54159572, 0.45840428]])"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.exp(x.T)/np.exp(x.T).sum(axis=0)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.27212356, 1.19811535, 1.29561577, 1.54462494, 1.7220336 ,\n",
       "        1.33187965, 1.22640504, 1.45458416, 2.45275835, 2.12290367],\n",
       "       [1.11644613, 1.54984518, 1.1910419 , 2.52774086, 1.1600523 ,\n",
       "        1.27674671, 2.08263388, 1.11313761, 1.38156244, 1.79681648]])"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(x.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.38856969, 2.74796054, 2.48665767, 4.0723658 , 2.88208591,\n",
       "       2.60862637, 3.30903891, 2.56772177, 3.83432079, 3.91972015])"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(x.T).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelK = Sequential()\n",
    "modelK.add(Dense(256, activation='relu', input_shape=(784,)))\n",
    "modelK.add(Dense(10, activation='softmax'))\n",
    "modelK.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 3ms/step - loss: 2.3678 - accuracy: 0.1138\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.3678197860717773, 0.11379999667406082]"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelK.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1/1 [==============================] - 0s 490ms/step - loss: 2.3686 - accuracy: 0.1132\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.3537 - accuracy: 0.1175\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 2.3393 - accuracy: 0.1224\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.3253 - accuracy: 0.1276\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.3117 - accuracy: 0.1338\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x27170ddac10>"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelK.fit(x_train, y_train, batch_size=60000, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "we = modelK.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.01072868, -0.07011654, -0.04292168, ..., -0.06528095,\n",
       "          0.07219957, -0.000182  ],\n",
       "        [-0.004351  ,  0.03537294,  0.05221342, ..., -0.00466982,\n",
       "          0.03416617, -0.00778186],\n",
       "        [-0.03104708,  0.06621596,  0.01865447, ...,  0.04406609,\n",
       "         -0.07491915, -0.00636803],\n",
       "        ...,\n",
       "        [-0.02563186,  0.00449546, -0.00927953, ..., -0.03727087,\n",
       "          0.05514884,  0.05221426],\n",
       "        [-0.01084206, -0.02047814, -0.03697304, ..., -0.02528916,\n",
       "          0.02550144,  0.07291465],\n",
       "        [-0.02149177,  0.07128689,  0.00808644, ...,  0.03317444,\n",
       "         -0.00483374,  0.0458691 ]], dtype=float32),\n",
       " array([-1.84047298e-04,  6.38793208e-05,  2.76196486e-04,  2.35976418e-04,\n",
       "         1.66774364e-04, -1.85569425e-05, -3.79865320e-04,  3.47005553e-05,\n",
       "         2.44475639e-04,  5.67853874e-07, -2.41147543e-04, -1.89210012e-04,\n",
       "        -1.83290576e-05, -4.82491712e-04, -4.00788886e-05,  8.42920781e-05,\n",
       "         9.57026787e-05, -5.61380526e-04,  6.39556019e-05,  4.34355723e-04,\n",
       "        -4.74045199e-04, -3.49286740e-04,  5.76219245e-05,  1.03224287e-04,\n",
       "         1.74938294e-04,  3.81921782e-05, -6.55220880e-04,  3.81777681e-05,\n",
       "        -1.11304093e-04,  1.85876386e-04, -1.00472505e-04,  2.14685177e-04,\n",
       "         3.60262144e-04,  4.04442020e-04,  1.15990188e-05, -3.43268795e-04,\n",
       "         4.79795912e-04, -3.52741568e-04,  2.70129356e-04,  2.46976299e-04,\n",
       "        -2.57046660e-04,  4.92881401e-04, -2.17514913e-04, -1.70067608e-04,\n",
       "        -5.62163223e-05, -2.66127347e-04, -1.03404564e-04, -2.04671218e-04,\n",
       "        -2.89246993e-04,  1.84563192e-04, -3.32008553e-04, -2.41910457e-04,\n",
       "         4.17259056e-04, -2.88983108e-04, -2.48145894e-04,  4.08805936e-05,\n",
       "        -2.99931999e-04, -6.80130033e-05,  8.99269944e-05,  6.50091912e-04,\n",
       "         1.10999761e-04,  2.71534955e-04, -1.73184744e-05, -1.60155396e-04,\n",
       "        -5.73201687e-04, -4.68414692e-05, -2.18161047e-04,  3.56551573e-05,\n",
       "        -4.92344807e-05, -9.75315415e-05,  3.92839662e-04, -8.24625677e-05,\n",
       "         4.25830003e-05,  2.77918327e-04, -1.34007632e-05,  3.37658712e-04,\n",
       "         1.33664013e-04, -2.95654143e-04, -1.73388486e-04, -1.26745086e-04,\n",
       "        -5.65260125e-04,  1.51370012e-04,  1.61688906e-04,  2.05451841e-04,\n",
       "        -3.06543399e-04, -7.14941416e-05, -2.17006294e-04, -7.08701555e-04,\n",
       "        -2.98523228e-04, -1.45518483e-04, -1.39288168e-04, -3.78347177e-04,\n",
       "        -2.38215129e-04, -1.47535166e-05,  3.19245737e-04, -1.04636681e-04,\n",
       "        -8.20350600e-04, -1.95845914e-05, -3.24015156e-04, -2.48012278e-04,\n",
       "        -2.22064191e-04, -8.85602087e-04, -7.97213434e-05,  1.63053337e-04,\n",
       "        -9.43264968e-05,  2.44173832e-04,  5.08565165e-04,  5.03347910e-05,\n",
       "         1.52315100e-04, -7.86276883e-04,  7.35194539e-04, -9.24491087e-06,\n",
       "        -1.77452079e-04,  2.14806962e-04,  2.58865795e-04, -1.29785491e-04,\n",
       "         4.12636145e-05,  5.15899039e-04,  5.32298116e-04, -8.08339741e-04,\n",
       "         6.18267368e-05,  2.45381470e-05, -5.52468526e-04, -5.60546061e-04,\n",
       "         5.17550972e-04, -1.05901301e-04,  1.23325386e-04, -1.75758163e-04,\n",
       "         1.37969691e-04, -3.90561654e-05, -3.16878606e-04,  4.60935582e-04,\n",
       "         8.07058968e-05, -3.35946766e-04,  9.29046291e-05,  8.11090740e-06,\n",
       "         2.34653067e-04, -1.15350209e-04, -3.90604924e-04,  8.74097459e-05,\n",
       "         5.07024488e-05, -4.53166786e-06, -4.94413776e-04, -1.33753420e-04,\n",
       "        -4.06808656e-04, -2.93187797e-04, -1.09695502e-04, -1.66983387e-04,\n",
       "        -3.53975105e-04, -4.41383127e-05,  5.48510707e-06,  3.68773850e-04,\n",
       "        -3.92781250e-04,  5.95055208e-05,  1.04061997e-04, -4.63136821e-05,\n",
       "        -1.45586862e-04, -3.77453369e-04,  1.64731347e-04,  5.20291760e-05,\n",
       "        -3.19970743e-04,  6.58428035e-05, -4.53477202e-04,  3.37744532e-05,\n",
       "         8.03190778e-05, -4.97867914e-05, -5.69615338e-04, -3.55259253e-04,\n",
       "        -3.93382245e-04,  4.52419568e-04,  3.21970219e-05, -2.69916345e-04,\n",
       "        -4.12347028e-04, -5.67033130e-04, -2.92084442e-04,  8.03114614e-04,\n",
       "        -6.58755831e-04,  1.00002682e-04, -4.17320116e-04,  7.13511326e-05,\n",
       "        -5.40729321e-04,  1.38818432e-04,  1.47297120e-04, -6.68156601e-04,\n",
       "        -6.16451958e-04,  1.89277285e-04, -2.08130150e-04, -7.75168010e-04,\n",
       "        -8.76031118e-05,  5.58283180e-04, -7.70721701e-04, -4.25411272e-04,\n",
       "        -1.52144567e-04, -5.18593151e-06, -3.99445358e-04, -1.04099236e-05,\n",
       "         1.44718128e-04,  3.10114910e-06, -4.88057529e-04,  1.57809322e-04,\n",
       "         1.20207231e-04,  1.65202058e-04, -2.40881724e-04, -3.47504858e-04,\n",
       "         7.67132879e-05, -5.03615447e-05,  1.36297691e-04, -2.11827082e-05,\n",
       "        -1.42027187e-04, -1.49292304e-04,  4.03490558e-04, -1.50389606e-04,\n",
       "        -4.67712554e-04, -1.80000556e-04,  1.15080620e-03,  2.70263408e-04,\n",
       "         2.67852913e-04,  1.62984652e-04,  4.16409457e-05, -6.51550945e-05,\n",
       "        -3.33425007e-04,  2.90038064e-04, -4.98017995e-04,  9.03521941e-05,\n",
       "        -5.58020911e-05, -4.65546269e-04,  2.61880661e-04,  4.80287745e-05,\n",
       "        -1.74159286e-04, -1.07999635e-03, -1.09697008e-04,  2.82367371e-04,\n",
       "         3.81467544e-04, -4.46058766e-05,  6.50439411e-04,  5.02689974e-04,\n",
       "         1.73281325e-04, -2.08757381e-04, -1.03639555e-04, -3.12823977e-04,\n",
       "         1.36055576e-04,  2.97811930e-04, -1.28193235e-04, -1.81775977e-04,\n",
       "         2.33024912e-04,  2.27728160e-04, -9.18768710e-05,  7.49867249e-05,\n",
       "        -1.57110553e-04, -6.57772878e-04, -2.45613046e-04,  2.01492832e-04,\n",
       "         7.51311745e-05,  4.40492644e-04, -9.89390974e-05, -7.87262979e-05],\n",
       "       dtype=float32),\n",
       " array([[-0.05062348,  0.03314626,  0.06425334, ..., -0.01530538,\n",
       "          0.00469751, -0.12938063],\n",
       "        [-0.12242049, -0.06850633, -0.0304223 , ..., -0.07210414,\n",
       "          0.07851667,  0.12419814],\n",
       "        [-0.03184846,  0.02400663, -0.09300938, ...,  0.12832732,\n",
       "          0.07780426, -0.01977064],\n",
       "        ...,\n",
       "        [-0.13326745,  0.08779313,  0.12556574, ...,  0.05722261,\n",
       "          0.10475675,  0.11573213],\n",
       "        [-0.07797499, -0.11670776, -0.11723287, ..., -0.12903085,\n",
       "         -0.09048641,  0.00668275],\n",
       "        [ 0.10610511,  0.03446591,  0.04272461, ..., -0.10769102,\n",
       "          0.02650121,  0.12754935]], dtype=float32),\n",
       " array([-0.00092945,  0.00118882, -0.00250392, -0.00022694, -0.00044813,\n",
       "         0.00085235, -0.00206772,  0.0021413 ,  0.00077492,  0.00121879],\n",
       "       dtype=float32)]"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "we"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.11834062,  0.47999893, -1.56807566, ...,  0.29614132,\n",
       "       -0.53334974,  0.69503652])"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.w_flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('RL-TF')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "465ce5a4eeb72cb0b708cdaa77c95cc8713430e9de06ab2846e19b6fca11a75a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
